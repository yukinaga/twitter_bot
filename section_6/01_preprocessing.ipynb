{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtrvm2yHv+J7GOdl2B+B24",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/twitter_bot/blob/master/section_6/01_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzAWLQ2a3io8"
      },
      "source": [
        "# データの前処理\n",
        "対話文のデータセットに前処理を行い、保存します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbeonHALP8fU"
      },
      "source": [
        "## ライブラリのインストール\n",
        "分かち書きのためにjanomeを、テキストデータの前処理のためにtorchtextをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUm6Vi2wQBfa"
      },
      "source": [
        "!pip install janome==0.4.1\n",
        "!pip install torchvision==0.7.0\n",
        "!pip install torchtext==0.7.0\n",
        "!pip install torch==1.6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcHOX9LyZc2g"
      },
      "source": [
        "## Google ドライブとの連携  \n",
        "以下のコードを実行し、認証コードを使用してGoogle ドライブをマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h7BA67Ed5wT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wnroZxn434X"
      },
      "source": [
        "## 対話文の取得\n",
        "雑談対話コーパス「projectnextnlp-chat-dialogue-corpus.zip」をダウンロードします。  \n",
        "  \n",
        "> Copyright (c) 2015 Project Next NLP 対話タスク 参加者一同  \n",
        "> https://sites.google.com/site/dialoguebreakdowndetection/chat-dialogue-corpus/LICENSE.txt  \n",
        "> Released under the MIT license\n",
        "\n",
        "解凍したフォルダをGoogle ドライブにアップします。  \n",
        "フォルダからjsonファイルを読み込み、対話文として成り立っている文章を取り出してリストに格納します。  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8diy0Nyz4cL1"
      },
      "source": [
        "import glob  # ファイルの取得に使用\n",
        "import json  # jsonファイルの読み込みに使用\n",
        "import re\n",
        "\n",
        "path = \"/content/drive/My Drive/live_ai_data/projectnextnlp-chat-dialogue-corpus/json\"  # フォルダの場所を指定\n",
        "\n",
        "files = glob.glob(path + \"/*/*.json\")  # ファイルの一覧\n",
        "dialogues = []  # 複数の対話文を格納するリスト\n",
        "file_count= 0  # ファイル数のカウント\n",
        "for file in files:\n",
        "    with open(file, \"r\") as f:\n",
        "        json_dic = json.load(f)\n",
        "        dialogue = []  # 単一の対話\n",
        "        for turn in json_dic[\"turns\"]:\n",
        "            annotations = turn[\"annotations\"]  # 注釈\n",
        "            speaker = turn[\"speaker\"]  # 発言者\n",
        "            utterance = turn[\"utterance\"]  # 発言\n",
        "\n",
        "            # 空の文章や、特殊文字や数字が含まれる文章は除く\n",
        "            if (utterance==\"\") or (\"\\\\u\" in utterance) or (re.search(\"\\d\", utterance)!=None):\n",
        "                dialogue.clear()  # 対話をリセット\n",
        "                continue\n",
        "\n",
        "            utterance = utterance.replace(\".\", \"。\").replace(\",\", \"、\")  # 全角\n",
        "            utterance = utterance.replace(\"．\", \"。\").replace(\"，\", \"、\")  # 半角\n",
        "            utterance = utterance.split(\"。\")[0]\n",
        "\n",
        "            if speaker==\"U\":  # 発言者が人間であれば\n",
        "                dialogue.append(utterance) \n",
        "            else:  # 発言者がシステムであれば\n",
        "                is_wrong = False\n",
        "                for annotation in annotations:\n",
        "                    breakdown = annotation[\"breakdown\"]  # 分類\n",
        "                    if breakdown==\"X\":  # 1つでも不適切評価があれば\n",
        "                        is_wrong = True\n",
        "                        break\n",
        "                if is_wrong:\n",
        "                    dialogue.clear()  # 対話をリセット\n",
        "                else:\n",
        "                    dialogue.append(utterance)  # 不適切評価が無ければ対話に追加\n",
        "            \n",
        "            if len(dialogue) >= 2:  # 単一の会話が成立すれば\n",
        "                dialogues.append(dialogue.copy())\n",
        "                dialogue.pop(0)  # 最初の要素を削除\n",
        "\n",
        "    file_count += 1\n",
        "    if file_count%100 == 0:\n",
        "        print(\"files:\", file_count, \"dialogues\", len(dialogues))\n",
        "\n",
        "print(\"files:\", file_count, \"dialogues\", len(dialogues))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdeh_-VsA-CC"
      },
      "source": [
        "## データ拡張の準備\n",
        "データ拡張の準備として、正規表現の設定および分かち書きを行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPk8KUboZTYV"
      },
      "source": [
        "import re\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "re_kanji = re.compile(r\"^[\\u4E00-\\u9FD0]+$\")  # 漢字の検出用\n",
        "re_katakana = re.compile(r\"[\\u30A1-\\u30F4]+\")  # カタカナの検出用\n",
        "j_tk = Tokenizer()\n",
        "\n",
        "def wakati(text):\n",
        "    return [tok for tok in j_tk.tokenize(text, wakati=True)] \n",
        "\n",
        "wakati_inp = []  # 単語に分割された入力文\n",
        "wakati_rep = []  # 単語に分割された応答文\n",
        "for dialogue in dialogues:\n",
        "    wakati_inp.append(wakati(dialogue[0])[:10])\n",
        "    wakati_rep.append(wakati(dialogue[1])[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az84f0hUDRam"
      },
      "source": [
        "## データ拡張\n",
        "対話データの数を水増しします。  \n",
        "ある入力文を、それに対応する応答文以外の複数の応答文と組み合わせます。  \n",
        "組み合わせる応答文は、入力文に含まれる漢字やカタカナの単語を含むものを選択します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MtsSsD00lW0"
      },
      "source": [
        "dialogues_plus = []\n",
        "for i, w_inp in enumerate(wakati_inp):  # 全ての入力文でループ\n",
        "    inp_count = 0  # ある入力から生成された対話文をカウント\n",
        "    for j, w_rep in enumerate(wakati_rep):  # 全ての応答文でループ\n",
        "        if i==j:\n",
        "            dialogues_plus.append([\"\".join(w_inp), \"\".join(w_rep)])\n",
        "            continue\n",
        "        similarity = 0  # 類似度\n",
        "        for w in w_inp:  # 入力文と同じ単語があり、それが漢字かカタカナであれば類似度を上げる\n",
        "            if (w in w_rep) and (re_kanji.fullmatch(w) or re_katakana.fullmatch(w)):\n",
        "                similarity += 1\n",
        "        if similarity >= 1:\n",
        "            dialogue_plus = [\"\".join(w_inp), \"\".join(w_rep)]\n",
        "            if dialogue_plus not in dialogues_plus:\n",
        "                dialogues_plus.append(dialogue_plus)\n",
        "                inp_count += 1\n",
        "                if inp_count >= 12:  # ある入力から生成する対話文の上限\n",
        "                    break\n",
        "\n",
        "    if i%1000 == 0:\n",
        "        print(\"i:\", i, \"dialogues_pus:\", len(dialogues_plus))\n",
        "\n",
        "print(\"i:\", i, \"dialogues_pus:\", len(dialogues_plus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T29_kUYcIUy7"
      },
      "source": [
        "拡張された対話データを、新たな対話データとします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_738pEiRC-F"
      },
      "source": [
        "dialogues = dialogues_plus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LADy70wOgyXg"
      },
      "source": [
        "## 対話データの保存\n",
        "対話データをcsvファイルとしてGoogle Driveに保存します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIyvN2MT4Unl"
      },
      "source": [
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dialogues_train, dialogues_test =  train_test_split(dialogues, shuffle=True, test_size=0.05)  # 5%がテストデータ\n",
        "path = \"/content/drive/My Drive/live_ai_data/\"  # 保存場所\n",
        "\n",
        "with open(path+\"dialogues_train.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(dialogues_train)\n",
        "\n",
        "with open(path+\"dialogues_test.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(dialogues_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0_B6zzsTJB"
      },
      "source": [
        "## 対話文の取得\n",
        "Googleドライブから、対話文のデータを取り出してデータセットに格納します。  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoPANR3zmi7m"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "path = \"/content/drive/My Drive/live_ai_data/\"  # 保存場所を指定\n",
        "\n",
        "j_tk = Tokenizer()\n",
        "def tokenizer(text): \n",
        "    return [tok for tok in j_tk.tokenize(text, wakati=True)]  # 内包表記\n",
        " \n",
        "# データセットの列を定義\n",
        "input_field = torchtext.data.Field(  # 入力文\n",
        "    sequential=True,  # データ長さが可変かどうか\n",
        "    tokenize=tokenizer,  # 前処理や単語分割などのための関数\n",
        "    batch_first=True,  # バッチの次元を先頭に\n",
        "    lower=True  # アルファベットを小文字に変換\n",
        "    )\n",
        "\n",
        "reply_field = torchtext.data.Field(  # 応答文\n",
        "    sequential=True,  # データ長さが可変かどうか\n",
        "    tokenize=tokenizer,  # 前処理や単語分割などのための関数\n",
        "    init_token = \"<sos>\",  # 文章開始のトークン\n",
        "    eos_token = \"<eos>\",  # 文章終了のトークン\n",
        "    batch_first=True,  # バッチの次元を先頭に\n",
        "    lower=True  # アルファベットを小文字に変換\n",
        "    )\n",
        " \n",
        "# csvファイルからデータセットを作成\n",
        "train_data, test_data = torchtext.data.TabularDataset.splits(\n",
        "    path=path,\n",
        "    train=\"dialogues_train.csv\",\n",
        "    validation=\"dialogues_test.csv\",\n",
        "    format=\"csv\",\n",
        "    fields=[(\"inp_text\", input_field), (\"rep_text\", reply_field)]  # 列の設定\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOpOvldJdMtl"
      },
      "source": [
        "## 単語とインデックスの対応\n",
        "単語にインデックスを割り振り、辞書として格納します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6QlG4w2ooXI"
      },
      "source": [
        "input_field.build_vocab(\n",
        "    train_data,\n",
        "    min_freq=3,\n",
        "    )\n",
        "reply_field.build_vocab(\n",
        "    train_data,\n",
        "    min_freq=3,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQdYRJBvBN2H"
      },
      "source": [
        "print(input_field.vocab.freqs)  # 各単語の出現頻度\n",
        "print(len(input_field.vocab.stoi))\n",
        "print(len(input_field.vocab.itos))\n",
        "print(len(reply_field.vocab.stoi))\n",
        "print(len(reply_field.vocab.itos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Qdx6zHgOAi"
      },
      "source": [
        "## データセットの保存\n",
        "データセットの`examples`とFieldをそれぞれ保存します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVCaFjoZAaGd"
      },
      "source": [
        "import dill\n",
        "\n",
        "torch.save(train_data.examples, path+\"train_examples.pkl\", pickle_module=dill)\n",
        "torch.save(test_data.examples, path+\"test_examples.pkl\", pickle_module=dill)\n",
        "\n",
        "torch.save(input_field, path+\"input_field.pkl\", pickle_module=dill)\n",
        "torch.save(reply_field, path+\"reply_field.pkl\", pickle_module=dill)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}