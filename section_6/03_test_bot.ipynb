{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_test_bot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOa+WH7vWghTdQj2vA3LOsR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/twitter_bot/blob/master/section_6/03_test_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6jgh_L_va9p"
      },
      "source": [
        "# ボットのテスト\n",
        "訓練済みのモデルを使用し、返答およびツイート生成のテストを行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbeonHALP8fU"
      },
      "source": [
        "## ライブラリのインストール\n",
        "分かち書きのためにjanomeを、テキストデータの前処理のためにtorchtextをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUm6Vi2wQBfa"
      },
      "source": [
        "!pip install janome==0.4.1\n",
        "!pip install torchvision==0.7.0\n",
        "!pip install torchtext==0.7.0\n",
        "!pip install torch==1.6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcHOX9LyZc2g"
      },
      "source": [
        "## Google ドライブとの連携  \n",
        "以下のコードを実行し、認証コードを使用してGoogle ドライブをマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h7BA67Ed5wT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENfclu7k9vtz"
      },
      "source": [
        "## モデルの定義\n",
        "モデルのパラメータを読み込む前に、Seq2Seqのモデルを定義します。  \n",
        "ほぼ、訓練時に使用したモデルと同じですが、`Seq2Seq`クラスの`predict`*メソッドで*`max_length`で応答文の最大長さを指定する点のみ異なります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGww-czaupGM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_h, n_vocab, n_emb, num_layers=1, bidirectional=False, dropout=0.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_h = n_h\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.dropout = dropout  # ドロップアウト層\n",
        "\n",
        "        # 埋め込み層\n",
        "        self.embedding = nn.Embedding(n_vocab, n_emb)\n",
        "        self.embedding_dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "        self.gru = nn.GRU(  # GRU層\n",
        "            input_size=n_emb,  # 入力サイズ\n",
        "            hidden_size=n_h,  # ニューロン数\n",
        "            batch_first=True,  # 入力を (バッチサイズ, 時系列の数, 入力の数) にする\n",
        "            num_layers=num_layers,  # RNN層の数（層を重ねることも可能）\n",
        "            bidirectional=bidirectional,  # Bidrectional RNN\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 文章の長さを取得\n",
        "        idx_pad = input_field.vocab.stoi[\"<pad>\"]\n",
        "        sentence_lengths = x.size()[1] - (x == idx_pad).sum(dim=1)\n",
        "\n",
        "        y = self.embedding(x)  # 単語をベクトルに変換\n",
        "        y = self.embedding_dropout(y)\n",
        "        y = nn.utils.rnn.pack_padded_sequence(  # 入力のパッキング\n",
        "            y,\n",
        "            sentence_lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "            )\n",
        "        y, h = self.gru(y)\n",
        "\n",
        "        y, _ = nn.utils.rnn.pad_packed_sequence(y, batch_first=True)  # テンソルに戻す\n",
        "        if self.bidirectional:  # 双方向の値を足し合わせる\n",
        "            y = y[:, :, :self.n_h] + y[:, :, self.n_h:]\n",
        "            h = h[:self.num_layers] + h[self.num_layers:]\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_h, n_out, n_vocab, n_emb, num_layers=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_h = n_h\n",
        "        self.n_out = n_out\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # 埋め込み層\n",
        "        self.embedding = nn.Embedding(n_vocab, n_emb)\n",
        "        self.embedding_dropout = nn.Dropout(self.dropout)  # ドロップアウト層\n",
        "\n",
        "        self.gru = nn.GRU(  # GRU層\n",
        "            input_size=n_emb,  # 入力サイズ\n",
        "            hidden_size=n_h,  # ニューロン数\n",
        "            batch_first=True,  # 入力を (バッチサイズ, 時系列の数, 入力の数) にする\n",
        "            num_layers=num_layers,  # RNN層の数（層を重ねることも可能）\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(n_h*2, self.n_out)  # コンテキストベクトルが合流するので2倍のサイズ\n",
        "                \n",
        "    def forward(self, x, h_encoder, y_encoder):\n",
        "        y = self.embedding(x)  # 単語をベクトルに変換\n",
        "        y = self.embedding_dropout(y)\n",
        "        y, h = self.gru(y, h_encoder)\n",
        "\n",
        "        # ----- Attension -----\n",
        "        y_tr = torch.transpose(y, 1, 2)  # 次元1と次元2を入れ替える\n",
        "        ed_mat = torch.bmm(y_encoder, y_tr)  # バッチごとに行列積\n",
        "        attn_weight = F.softmax(ed_mat, dim=1)  # attension weightの計算\n",
        "        attn_weight_tr = torch.transpose(attn_weight, 1, 2)  # 次元1と次元2を入れ替える\n",
        "        context = torch.bmm(attn_weight_tr, y_encoder)  # コンテキストベクトルの計算\n",
        "        y = torch.cat([y, context], dim=2)  # 出力とコンテキストベクトルの合流\n",
        "\n",
        "        y = self.fc(y)\n",
        "        y = F.softmax(y, dim=2)\n",
        "        \n",
        "        return y, h\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, is_gpu=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.is_gpu = is_gpu\n",
        "        if self.is_gpu:\n",
        "            self.encoder.cuda()\n",
        "            self.decoder.cuda()\n",
        "        \n",
        "    def forward(self, x_encoder, x_decoder):  # 訓練に使用\n",
        "        if self.is_gpu:\n",
        "            x_encoder, x_decoder = x_encoder.cuda(), x_decoder.cuda()\n",
        "\n",
        "        batch_size = x_decoder.shape[0]\n",
        "        n_time = x_decoder.shape[1]\n",
        "        y_encoder, h = self.encoder(x_encoder)\n",
        "\n",
        "        y_decoder = torch.zeros(batch_size, n_time, self.decoder.n_out)\n",
        "        if self.is_gpu:\n",
        "            y_decoder = y_decoder.cuda()\n",
        "\n",
        "        for t in range(0, n_time):\n",
        "            x = x_decoder[:, t:t+1]  # Decoderの入力を使用\n",
        "            y, h= self.decoder(x, h, y_encoder)\n",
        "            y_decoder[:, t:t+1, :] = y\n",
        "        return y_decoder\n",
        "\n",
        "    def predict(self, x_encoder, max_length=10):  # 予測に使用\n",
        "        if self.is_gpu:\n",
        "            x_encoder = x_encoder.cuda()\n",
        "\n",
        "        batch_size = x_encoder.shape[0]\n",
        "        n_time = max_length\n",
        "        y_encoder, h = self.encoder(x_encoder)\n",
        "\n",
        "        y_decoder = torch.zeros(batch_size, n_time, dtype=torch.long)\n",
        "        if self.is_gpu:\n",
        "            y_decoder = y_decoder.cuda() \n",
        "\n",
        "        y = torch.ones(batch_size, 1, dtype=torch.long) * input_field.vocab.stoi[\"<sos>\"]\n",
        "        for t in range(0, n_time):\n",
        "            x = y  # 前の時刻の出力を入力に\n",
        "            if self.is_gpu:\n",
        "                x = x.cuda()\n",
        "            y, h= self.decoder(x, h, y_encoder)\n",
        "            y = y.argmax(2)\n",
        "            y_decoder[:, t:t+1] = y  \n",
        "        return y_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAe1taxDAyVx"
      },
      "source": [
        "## モデルの読み込み\n",
        "保存されたパラメータを読み込み、モデルに設定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91g6dTvgwgOK"
      },
      "source": [
        "import dill\n",
        "\n",
        "path = \"/content/drive/My Drive/live_ai_data/\"\n",
        "\n",
        "input_field = torch.load(path+\"input_field.pkl\", pickle_module=dill)\n",
        "reply_field = torch.load(path+\"reply_field.pkl\", pickle_module=dill)\n",
        "\n",
        "is_gpu = False  # GPUを使用するかどうか\n",
        "n_h = 896\n",
        "n_vocab_inp = len(input_field.vocab.itos)\n",
        "n_vocab_rep = len(reply_field.vocab.itos)\n",
        "n_emb = 300\n",
        "n_out = n_vocab_rep\n",
        "early_stop_patience = 5  # 早期終了のタイミング（誤差の最小値が何回更新されなかったら終了か）\n",
        "num_layers = 1\n",
        "bidirectional = True\n",
        "dropout = 0.0\n",
        "clip = 100\n",
        "\n",
        "encoder = Encoder(n_h, n_vocab_inp, n_emb, num_layers, bidirectional, dropout=dropout)\n",
        "decoder = Decoder(n_h, n_out, n_vocab_rep, n_emb, num_layers, dropout=dropout)\n",
        "seq2seq = Seq2Seq(encoder, decoder, is_gpu=is_gpu)\n",
        "\n",
        "seq2seq.load_state_dict(torch.load(path+\"model_bot.pth\", map_location=torch.device(\"cpu\")))  #CPU対応"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z-FapdIBFQ4"
      },
      "source": [
        "## 応答文の生成\n",
        "janomeのtokenizerを設定し、応答文を生成する関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ter0y6eXnAFi"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "j_tk = Tokenizer()\n",
        "\n",
        "def reply(inp_text, tokenizer, max_length=10):\n",
        "    words = [tok for tok in tokenizer.tokenize(inp_text, wakati=True)]  # 分かち書き\n",
        "\n",
        "    # 入力を単語idの並びに変換\n",
        "    word_ids = []\n",
        "    for word in words:\n",
        "        idx = input_field.vocab.stoi[word]\n",
        "        word_ids.append(idx)\n",
        "\n",
        "    x = torch.tensor(word_ids)\n",
        "    x = x.view(1, -1)  # バッチ対応\n",
        "    y = seq2seq.predict(x, max_length)\n",
        "\n",
        "    # 応答文の作成\n",
        "    rep_text = \"\"\n",
        "    for j in range(y.size()[1]):\n",
        "        word = reply_field.vocab.itos[y[0][j]]\n",
        "        if word==\"<eos>\":\n",
        "            break\n",
        "        rep_text += word\n",
        "\n",
        "    # トークンの削除\n",
        "    rep_text = rep_text.replace(\"<sos>\", \"\")\n",
        "    rep_text = rep_text.replace(\"<eos>\", \"\")\n",
        "    rep_text = rep_text.replace(\"<pad>\", \"\")\n",
        "    rep_text = rep_text.replace(\"<unk>\", \"\")\n",
        "\n",
        "    return rep_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w2PV0LRnJDO"
      },
      "source": [
        "入力文に対応する応答文を生成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahh6fL8QwvMK"
      },
      "source": [
        "print(\"ボットに話しかけてください。\")\n",
        "inp_text = input()  # ユーザーの入力を取得\n",
        "inp_text = inp_text.replace(\".\", \"。\").replace(\",\", \"、\")  # 全角\n",
        "inp_text = inp_text.replace(\"．\", \"。\").replace(\"，\", \"、\")  # 半角\n",
        "\n",
        "print(\"input:\", inp_text)\n",
        "print(\"reply:\", reply(inp_text, j_tk, max_length=20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYcGCpVeRIW"
      },
      "source": [
        "## Twitterの各設定\n",
        "### APIのkey\n",
        "Twitter Developer PlatformからAPI Key & SecretおよびAccess Token & Secretを取得し、以下のセルの変数に設定します。\n",
        "```\n",
        "api_key =\"API key\"\n",
        "api_key_secret =\"API key secret\"\n",
        "access_token=\"Access token\"\n",
        "access_token_secret =\"Access token secret\"\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU2eD2eEeVl2"
      },
      "source": [
        "api_key =\n",
        "api_key_secret =\n",
        "access_token =\n",
        "access_token_secret ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi6S4e5gkXAA"
      },
      "source": [
        "Tweepyを利用し、OAuth認証にアクセストークンを設定します。  \n",
        "その上で、Twitterとやり取りするためのAPIを設定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpm1fb6nkV5_"
      },
      "source": [
        "import tweepy\n",
        "\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "user_name = \"Twitterのユーザー名\"  # @を除いたTwitterのユーザー名"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xq02cUBZotU"
      },
      "source": [
        "## 返答機能の実装\n",
        "Twitterからユーザーへのリプライを取得し、返答を返す関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRtyoqlpaF09"
      },
      "source": [
        "import re  # 正規表現\n",
        "\n",
        "def reply_tweet(s_id):\n",
        "    try:\n",
        "        tweets = api.mentions_timeline(since_id=s_id)  #メンションを取得\n",
        "    except tweepy.error.TweepError as e:\n",
        "        print(e)\n",
        "        return\n",
        "\n",
        "    for tweet in tweets:\n",
        "        if tweet.user.screen_name==user_name:\n",
        "            continue\n",
        "\n",
        "        inp_text = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+\", \"\", tweet.text)  # URLを削除\n",
        "        inp_text = re.sub(\"@[^\\s]+\", \"\", inp_text)  # @ユーザー名 を削除\n",
        "\n",
        "        rep_text = reply(inp_text, j_tk, max_length=20)\n",
        "        if rep_text==\"\":\n",
        "            continue\n",
        "        rep_text = \"@\" + tweet.user.screen_name + \" \" + rep_text\n",
        "\n",
        "        print(\"user:\", tweet.user.screen_name)\n",
        "        print(\"input:\", tweet.text)\n",
        "        print(\"reply:\", rep_text)\n",
        "\n",
        "        try:\n",
        "            api.update_status(rep_text, tweet.id)  # 投稿\n",
        "        except tweepy.error.TweepError as e:\n",
        "            print(e)\n",
        "\n",
        "        s_id = tweet.id if tweet.id>s_id else s_id  # since_idを更新\n",
        "\n",
        "    return s_id  # 最新のメンションのidを返す"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMEN48xHFrlZ"
      },
      "source": [
        "定義された関数を使って返答を返します。  \n",
        "取得した最新のTweetのid、`since_id`はファイルに保存します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0W0TCLEwK-t"
      },
      "source": [
        "import os \n",
        "\n",
        "if os.path.exists(\"since_id.txt\"):\n",
        "    with open(\"since_id.txt\", \"r\") as f:\n",
        "        since_id = int(f.read())\n",
        "else:\n",
        "    since_id = 1000000000000000000  # これ以降のツイートidを取得する\n",
        "\n",
        "since_id = reply_tweet(since_id)\n",
        "with open(\"since_id.txt\", \"w\") as f:\n",
        "    f.write(str(since_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0VBx4WGF88L"
      },
      "source": [
        "## Tweetの投稿\n",
        "Tweetを投稿する関数を定義します。  \n",
        "Tweetは、トレンドを含む文とそれに対する応答から生成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOQedZOYCxNy"
      },
      "source": [
        "import random\n",
        "\n",
        "def tweet(woeid):\n",
        "    try:\n",
        "        trends = api.trends_place(id = woeid)[0]  # トレンドを取得\n",
        "    except tweepy.error.TweepError as e:\n",
        "        print(e)\n",
        "        return\n",
        "    trends = trends[\"trends\"]\n",
        "    trend = random.sample(trends, 2)  # トレンドを2つランダムに取得\n",
        "\n",
        "    inp_text1 = re.sub(\"#\", \"\", trend[0][\"name\"]) + \"がトレンドです。\"\n",
        "    rep_text1 = reply(inp_text1, j_tk, max_length=20)\n",
        "    inp_text2 = re.sub(\"#\", \"\", trend[1][\"name\"]) + \"が流行ってます。\"\n",
        "    rep_text2 = reply(inp_text2, j_tk, max_length=20)\n",
        "\n",
        "    tw_text = inp_text1 + rep_text1 + \"\\n\" + inp_text2 + rep_text2\n",
        "    print(\"tweet:\", tw_text)\n",
        "    try:\n",
        "        api.update_status(tw_text)  # 投稿\n",
        "    except tweepy.error.TweepError as e:\n",
        "        print(e)\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106492fqhRKW"
      },
      "source": [
        "定義された関数を使ってTweetを投稿します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udW2zsq3EiPm"
      },
      "source": [
        "woeid = 23424856  # 日本を表すid\n",
        "tweet(woeid)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
