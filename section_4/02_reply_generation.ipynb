{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_reply_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOQws/Wt9vZ5ZajPSfvzs12",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/twitter_bot/blob/master/section_4/02_reply_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzAWLQ2a3io8"
      },
      "source": [
        "# seq2seqモデルの訓練\n",
        "対話文のデータセットを使って、Seq2Seqのモデルを訓練します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbeonHALP8fU"
      },
      "source": [
        "## ライブラリのインストール\n",
        "分かち書きのためにjanomeを、テキストデータの前処理のためにtorchtextをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUm6Vi2wQBfa"
      },
      "source": [
        "!pip install janome==0.4.1\n",
        "!pip install torchvision==0.7.0\n",
        "!pip install torchtext==0.7.0\n",
        "!pip install torch==1.6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcHOX9LyZc2g"
      },
      "source": [
        "## Google ドライブとの連携  \n",
        "以下のコードを実行し、認証コードを使用してGoogle ドライブをマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h7BA67Ed5wT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0_B6zzsTJB"
      },
      "source": [
        "## 対話文の取得\n",
        "Googleドライブから、対話文のデータを取り出してデータセットを作成します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoPANR3zmi7m"
      },
      "source": [
        "import torchtext\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "path = \"/content/drive/My Drive/live_ai_data/\"  # 保存場所を指定\n",
        "\n",
        "j_tk = Tokenizer()\n",
        "def tokenizer(text): \n",
        "    return [tok for tok in j_tk.tokenize(text, wakati=True)]  # 内包表記\n",
        " \n",
        "# データセットの列を定義\n",
        "input_field = torchtext.data.Field(  # 入力文\n",
        "    sequential=True,  # データ長さが可変かどうか\n",
        "    tokenize=tokenizer,  # 前処理や単語分割などのための関数\n",
        "    batch_first=True,  # バッチの次元を先頭に\n",
        "    lower=True  # アルファベットを小文字に変換\n",
        "    )\n",
        "\n",
        "reply_field = torchtext.data.Field(  # 応答文\n",
        "    sequential=True,  # データ長さが可変かどうか\n",
        "    tokenize=tokenizer,  # 前処理や単語分割などのための関数\n",
        "    init_token = \"<sos>\",  # 文章開始のトークン\n",
        "    eos_token = \"<eos>\",  # 文章終了のトークン\n",
        "    batch_first=True,  # バッチの次元を先頭に\n",
        "    lower=True  # アルファベットを小文字に変換\n",
        "    )\n",
        " \n",
        "# csvファイルからデータセットを作成\n",
        "train_data, test_data = torchtext.data.TabularDataset.splits(\n",
        "    path=path,\n",
        "    train=\"dialogues_train.csv\",\n",
        "    validation=\"dialogues_test.csv\",\n",
        "    format=\"csv\",\n",
        "    fields=[(\"inp_text\", input_field), (\"rep_text\", reply_field)]  # 列の設定\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev7dNYtr_GIV"
      },
      "source": [
        "## データセットの内容を表示\n",
        "データセットの内容を一部表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmi0Wvog_CcO"
      },
      "source": [
        "for example in train_data.examples[:10]:\n",
        "    print(example.inp_text, example.rep_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOpOvldJdMtl"
      },
      "source": [
        "## 単語とインデックスの対応\n",
        "単語にインデックスを割り振り、辞書として格納します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6QlG4w2ooXI"
      },
      "source": [
        "input_field.build_vocab(\n",
        "    train_data,\n",
        "    min_freq=2,\n",
        "    )\n",
        "reply_field.build_vocab(\n",
        "    train_data,\n",
        "    min_freq=2,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzuRAl8kv9tM"
      },
      "source": [
        "print(input_field.vocab.freqs)  # 各単語の出現頻度\n",
        "print()\n",
        "print(input_field.vocab.stoi)  # 　キーが単語 、値がインデックスの辞書（入力）\n",
        "print()\n",
        "print(input_field.vocab.itos)  # 　単語がインデックス順に格納されたリスト（入力）\n",
        "print()\n",
        "print(reply_field.vocab.stoi)  # 　キーが単語 、値がインデックスの辞書（応答）\n",
        "print()\n",
        "print(reply_field.vocab.itos)  # 　単語がインデックス順に格納されたリスト（応答）"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiohzweSnFYZ"
      },
      "source": [
        "## Iteratorの設定\n",
        "バッチごとに学習を行うために、Iteratorを設定します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sMWHKzjTZkZ"
      },
      "source": [
        "# Iteratorの設定\n",
        "batch_size = 32\n",
        "\n",
        "train_iterator = torchtext.data.Iterator(\n",
        "    train_data,\n",
        "    batch_size=batch_size, \n",
        "    train=True  # シャッフルして取り出す\n",
        ")\n",
        "\n",
        "test_iterator = torchtext.data.Iterator(\n",
        "    test_data,\n",
        "    batch_size=batch_size, \n",
        "    train=False,\n",
        "    sort=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMR9rJHpEaK8"
      },
      "source": [
        "ミニバッチを取り出して、内容を表示します。  \n",
        "ミニバッチには、単語をインデックスに置き換えた文章が格納されます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_NFDwZopAbr"
      },
      "source": [
        "batch = next(iter(train_iterator))  # ミニバッチを取り出す\n",
        "print(batch.inp_text.size())  # ミニバッチにおける入力のサイズ\n",
        "print(batch.inp_text[0])  # 最初の要素\n",
        "print(batch.rep_text.size())  # ミニバッチにおける応答のサイズ\n",
        "print(batch.rep_text[0])  # 最初の要素"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L4JUIxfiXIy"
      },
      "source": [
        "## Encoderのクラス\n",
        "Encoderをクラスとして実装します。  \n",
        "RNN部分にはGRUを使用します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj85SCNAmzGC"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_h, n_vocab, n_emb):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_h = n_h\n",
        "\n",
        "        # 埋め込み層\n",
        "        self.embedding = nn.Embedding(n_vocab, n_emb)\n",
        "\n",
        "        self.gru = nn.GRU(  # GRU層\n",
        "            input_size=n_emb,  # 入力サイズ\n",
        "            hidden_size=n_h,  # ニューロン数\n",
        "            batch_first=True,  # 入力を (バッチサイズ, 時系列の数, 入力の数) にする\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.embedding(x)  # 単語をベクトルに変換\n",
        "        y, h = self.gru(y)\n",
        "        return y, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYCATQ-NpexE"
      },
      "source": [
        "## Decoderのクラス\n",
        "Decoderをクラスとして実装します。  \n",
        "RNN部分にはGRUを使用します。  \n",
        "GRU層の出力は、全結合層を経てDecoderの出力となります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4_k1Lnypm9v"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_h, n_out, n_vocab, n_emb):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_h = n_h\n",
        "        self.n_out = n_out\n",
        "\n",
        "        # 埋め込み層\n",
        "        self.embedding = nn.Embedding(n_vocab, n_emb)\n",
        "\n",
        "        self.gru = nn.GRU(  # GRU層\n",
        "            input_size=n_emb,  # 入力サイズ\n",
        "            hidden_size=n_h,  # ニューロン数\n",
        "            batch_first=True,  # 入力を (バッチサイズ, 時系列の数, 入力の数) にする\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(n_h, n_out)\n",
        "                \n",
        "    def forward(self, x, h_encoder):\n",
        "        y = self.embedding(x)  # 単語をベクトルに変換\n",
        "        y, h = self.gru(y, h_encoder)\n",
        "        y = self.fc(y)\n",
        "        y = F.softmax(y, dim=2)\n",
        "        return y, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz1mobSD4J_K"
      },
      "source": [
        "## Seq2Seqのクラス\n",
        "Seq2Seqを構築します。  \n",
        "`is_gpu`が`True`であれば、GPU対応を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaP3V80wwF_b"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, is_gpu=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.is_gpu = is_gpu\n",
        "        if self.is_gpu:\n",
        "            self.encoder.cuda()\n",
        "            self.decoder.cuda()\n",
        "        \n",
        "    def forward(self, x_encoder, x_decoder):  # 訓練に使用\n",
        "        if self.is_gpu:\n",
        "            x_encoder, x_decoder = x_encoder.cuda(), x_decoder.cuda()\n",
        "\n",
        "        batch_size = x_decoder.shape[0]\n",
        "        n_time = x_decoder.shape[1]\n",
        "        y, h = self.encoder(x_encoder)\n",
        "\n",
        "        y_decoder = torch.zeros(batch_size, n_time, self.decoder.n_out)\n",
        "        if self.is_gpu:\n",
        "            y_decoder = y_decoder.cuda()\n",
        "\n",
        "        for t in range(0, n_time):\n",
        "            x = x_decoder[:, t:t+1]  # Decoderの入力を使用\n",
        "            y, h= self.decoder(x, h)\n",
        "            y_decoder[:, t:t+1, :] = y  \n",
        "        return y_decoder\n",
        "\n",
        "    def predict(self, x_encoder):  # 予測に使用\n",
        "        if self.is_gpu:\n",
        "            x_encoder = x_encoder.cuda()\n",
        "\n",
        "        batch_size = x_encoder.shape[0]\n",
        "        n_time = x_encoder.shape[1]\n",
        "        y, h = self.encoder(x_encoder)\n",
        "\n",
        "        y_decoder = torch.zeros(batch_size, n_time, dtype=torch.long)\n",
        "        if self.is_gpu:\n",
        "            y_decoder = y_decoder.cuda() \n",
        "\n",
        "        y = torch.ones(batch_size, 1, dtype=torch.long) * input_field.vocab.stoi[\"<sos>\"]\n",
        "        for t in range(0, n_time):\n",
        "            x = y  # 前の時刻の出力を入力に\n",
        "            if self.is_gpu:\n",
        "                x = x.cuda()\n",
        "            y, h= self.decoder(x, h)\n",
        "            y = y.argmax(2)\n",
        "            y_decoder[:, t:t+1] = y  \n",
        "        return y_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LlU7bHhC0Fj"
      },
      "source": [
        "## 学習\n",
        "DataLoaderを使ってミニバッチを取り出し、Seq2Seqのモデルを訓練します。  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc0297QKC01y"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "is_gpu = True  # GPUを使用するかどうか\n",
        "n_h = 512\n",
        "n_vocab = len(reply_field.vocab.stoi)\n",
        "n_emb = 300\n",
        "n_out = n_vocab\n",
        "\n",
        "# Seq2Seqのモデルを構築\n",
        "encoder = Encoder(n_h, n_vocab, n_emb)\n",
        "decoder = Decoder(n_h, n_out, n_vocab, n_emb)\n",
        "seq2seq = Seq2Seq(encoder, decoder, is_gpu=is_gpu)\n",
        "\n",
        "# 誤差関数\n",
        "loss_fnc = nn.CrossEntropyLoss()\n",
        "\n",
        "# 最適化アルゴリズム\n",
        "optimizer = optim.Adam(seq2seq.parameters(), lr=0.001)\n",
        "\n",
        "# 損失のログ\n",
        "record_loss_train = []\n",
        "record_loss_test = []\n",
        "\n",
        "# 学習\n",
        "for i in range(200):\n",
        "    # 訓練モード\n",
        "    seq2seq.train()\n",
        "\n",
        "    loss_train = 0\n",
        "    for j, batch in enumerate(train_iterator):\n",
        "        inp, rep = batch.inp_text, batch.rep_text\n",
        "        x_enc = inp\n",
        "        x_dec = rep[:, :-1]\n",
        "        y_dec = seq2seq(x_enc, x_dec)\n",
        "\n",
        "        t_dec = rep[:, 1:]\n",
        "        t_dec = t_dec.cuda() if is_gpu else t_dec\n",
        "        loss = loss_fnc(\n",
        "            y_dec.view(-1, y_dec.size()[2]),\n",
        "            t_dec.view(-1)\n",
        "            )\n",
        "        loss_train += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if j%100==0:\n",
        "            print(\"batch:\", str(j)+\"/\"+str(len(train_data)//batch_size+1), \"loss:\", loss.item())\n",
        "    loss_train /= j+1\n",
        "    record_loss_train.append(loss_train)\n",
        "\n",
        "    # 評価モード\n",
        "    seq2seq.eval()\n",
        "\n",
        "    loss_test = 0\n",
        "    for j, batch in enumerate(test_iterator):\n",
        "        inp, rep = batch.inp_text, batch.rep_text\n",
        "        x_enc = inp\n",
        "        x_dec = torch.ones(rep.size(), dtype=torch.long) * reply_field.vocab.stoi[\"<sos>\"]\n",
        "        x_dec[:, 1:] = rep[:, :-1]\n",
        "        y_dec = seq2seq(x_enc, x_dec)\n",
        "\n",
        "        t_dec = rep.cuda() if is_gpu else rep\n",
        "        loss = loss_fnc(\n",
        "            y_dec.view(-1, y_dec.size()[2]),\n",
        "            t_dec.view(-1)\n",
        "            )\n",
        "        loss_test += loss.item()\n",
        "    loss_test /= j+1\n",
        "    record_loss_test.append(loss_test)\n",
        "\n",
        "    if i%1 == 0:\n",
        "        print(\"Epoch:\", i, \"Loss_Train:\", loss_train, \"Loss_Test:\", loss_test)\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJwwrWTw43rx"
      },
      "source": [
        "## 誤差の推移\n",
        "誤差の推移をグラフ表示します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaJx4swE45XI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(record_loss_train)), record_loss_train, label=\"Train\")\n",
        "plt.plot(range(len(record_loss_test)), record_loss_test, label=\"Test\")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHyViGdX3goe"
      },
      "source": [
        "## 訓練済みのモデルを使用\n",
        "訓練済みのモデルを使用して、応答文を生成します。 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2AzROHf3goe"
      },
      "source": [
        "seq2seq.eval()  # 評価モード\n",
        "\n",
        "batch = next(iter(test_iterator))\n",
        "x = batch.inp_text\n",
        "y = seq2seq.predict(x)\n",
        "print(y[0])\n",
        "for i in range(x.size()[0]):\n",
        "    inp_text = \"\"\n",
        "    for j in range(x.size()[1]):\n",
        "        inp_text += input_field.vocab.itos[x[i][j]]\n",
        "\n",
        "    rep_text = \"\"\n",
        "    for j in range(y.size()[1]):\n",
        "        rep_text += reply_field.vocab.itos[y[i][j]]\n",
        "\n",
        "    print(\"input:\", inp_text)\n",
        "    print(\"reply:\", rep_text)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zhOuS914nOS"
      },
      "source": [
        "## 問題点\n",
        "\n",
        "*   Encoderへの入力には、バッチ内で要素数を揃えるために多数のパディング`<pad>`が末尾に含まれている。\n",
        "*   誤差の計算に、文章の終了`<eos>`と、その後の`<pad>`が使われている。\n",
        "*   過学習の問題。"
      ]
    }
  ]
}